# Sistema de Preguntas y Respuestas con RAG (Retrieval-Augmented Generation)

Este proyecto implementa un sistema de preguntas y respuestas usando RAG con LangChain. Permite la carga de documentos, su vectorizaciÃ³n y la consulta semÃ¡ntica mediante modelos LLM (OpenAI o modelo local).

## ğŸš€ TecnologÃ­as utilizadas

- FastAPI: Backend para la API REST.
- Streamlit: Frontend interactivo para usuarios.
- LangChain: Para la orquestaciÃ³n de cadenas RAG.
- ChromaDB: Base vectorial para almacenamiento y recuperaciÃ³n semÃ¡ntica.
- OpenAI: Modelos GPT-4o, GPT-4o-mini.
- Microsoft Phi-4 Mini: Modelo local para ejecuciÃ³n privada.
- HuggingFace Transformers: Para cargar modelos LLM y tokenizadores.
- Sentence Transformers: Para embeddings multilingÃ¼es.

## ğŸ§© CaracterÃ­sticas

- Ingesta de documentos (.csv, .xlsx) | Posibilidad de agregar docx, pdf a futuro
- Consulta semÃ¡ntica con RAG (Retriever + LLM)
- SelecciÃ³n entre modelo local o OpenAI desde el frontend
- SimulaciÃ³n de autenticaciÃ³n bÃ¡sica de usuario
- Historial de sesiones por usuario

## ğŸ“ Estructura del proyecto
```bash
.
â”œâ”€â”€ app/                  # Frontend (Streamlit)
â”‚   â”œâ”€â”€ app_front.py
â”‚   â”œâ”€â”€ sidebar.py
â”‚   â””â”€â”€ login.py
â”œâ”€â”€ api/                  # Backend y lÃ³gica de negocio
â”‚   â”œâ”€â”€ main.py           # Endpoints FastAPI
â”‚   â”œâ”€â”€ langchain_utils.py
â”‚   â”œâ”€â”€ db_utils.py       # AutenticaciÃ³n y logs
â”‚   â”œâ”€â”€ chroma_utils.py   # Vector store
â”‚   â””â”€â”€ api_utils.py      # Cliente HTTP
â”œâ”€â”€ documents/            # Archivos cargados
â”œâ”€â”€ embeddings/           # Vector store local
â”œâ”€â”€ .env                  # Claves y configuraciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ InstalaciÃ³n y ejecuciÃ³n local

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # o venv\Scripts\activate en Windows

# Instalar dependencias
pip install -r requirements.txt

# Variables de entorno
cp .env.example .env
# Edita .env y agrega tu OPENAI_API_KEY si usarÃ¡s OpenAI

# Iniciar backend (API)
uvicorn api.main:app --reload

# Iniciar frontend
streamlit run app/app_front.py
```

## ğŸ” Requisitos del modelo local (Phi-4 Mini)

- RAM: â†’ mÃ­nimo 8GB (ideal 16GB)
- GPU: â†’ NVIDIA con al menos 4GB VRAM (opcional, pero mejora rendimiento)
- Dependencias: transformers, torch, accelerate